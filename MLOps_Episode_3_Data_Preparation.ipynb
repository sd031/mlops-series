{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419d578d",
   "metadata": {},
   "source": [
    "# MLOps Episode 3: Data Extraction, Validation & Preparation\n",
    "\n",
    "In this episode, we focus on three critical steps of data preparation for machine learning models:\n",
    "\n",
    "1. **Data Extraction**: Retrieving data from various sources.\n",
    "2. **Data Validation**: Ensuring data quality and integrity.\n",
    "3. **Data Preparation**: Transforming the data for analysis.\n",
    "\n",
    "These steps are essential for ensuring the data used in machine learning pipelines is accurate, relevant, and ready for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ce049",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "\n",
    "Data extraction involves retrieving data from sources such as databases, APIs, flat files, or web scraping. Important considerations include:\n",
    "\n",
    "- **Source Identification**: Determine where the data resides.\n",
    "- **Data Formats**: Understand formats (CSV, JSON, XML, etc.).\n",
    "- **Automation**: Use scripts to streamline the process.\n",
    "\n",
    "### Sample Data\n",
    "We'll start with a simple CSV file:\n",
    "\n",
    "```csv\n",
    "user_id,name,email,age\n",
    "1,John Doe,john.doe@example.com,28\n",
    "2,Jane Smith,jane.smith@example.com,34\n",
    "3,Sam Johnson,sam.johnson@example.com,22\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV data\n",
    "data = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3],\n",
    "    'name': ['John Doe', 'Jane Smith', 'Sam Johnson'],\n",
    "    'email': ['john.doe@example.com', 'jane.smith@example.com', 'sam.johnson@example.com'],\n",
    "    'age': [28, 34, 22]\n",
    "})\n",
    "\n",
    "# Display extracted data\n",
    "print(\"Extracted Data:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bae1a",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "\n",
    "After extraction, validating the data ensures quality and consistency. Important steps include:\n",
    "\n",
    "- **Schema Validation**: Ensure proper data types.\n",
    "- **Missing Values**: Handle missing entries.\n",
    "- **Outlier Detection**: Detect and manage outliers.\n",
    "- **Consistency Checks**: Ensure consistency across sources.\n",
    "\n",
    "### Sample Data with Issues\n",
    "\n",
    "```csv\n",
    "user_id,name,email,age\n",
    "1,John Doe,john.doe@example.com,28\n",
    "2,Jane Smith,jane.smith@example.com,\n",
    "3,Sam Johnson,,22\n",
    "4,Emily Davis,emily.davis@example.com,120\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing issues in data\n",
    "data_with_issues = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3, 4],\n",
    "    'name': ['John Doe', 'Jane Smith', 'Sam Johnson', 'Emily Davis'],\n",
    "    'email': ['john.doe@example.com', 'jane.smith@example.com', None, 'emily.davis@example.com'],\n",
    "    'age': [28, None, 22, 120]\n",
    "})\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data_with_issues.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Outlier detection\n",
    "outliers = data_with_issues[(data_with_issues['age'] > 100) | (data_with_issues['age'] < 0)]\n",
    "print(\"Outliers:\\n\", outliers)\n",
    "\n",
    "# Basic schema validation\n",
    "print(\"Data Types:\\n\", data_with_issues.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726e534e",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This phase transforms validated data for analysis:\n",
    "\n",
    "- **Data Cleaning**: Remove duplicates and handle errors.\n",
    "- **Feature Engineering**: Create or adjust features for better model performance.\n",
    "- **Normalization and Scaling**: Adjust data for better convergence.\n",
    "- **Splitting the Data**: Create training, validation, and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ac97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cleaning data: Remove duplicates and fill missing values\n",
    "data_cleaned = data_with_issues.drop_duplicates()\n",
    "data_cleaned['age'].fillna(data_cleaned['age'].median(), inplace=True)\n",
    "\n",
    "# Normalize the age feature\n",
    "scaler = MinMaxScaler()\n",
    "data_cleaned['age_scaled'] = scaler.fit_transform(data_cleaned[['age']])\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "train, test = train_test_split(data_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(train)\n",
    "print(\"Testing Data:\")\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
